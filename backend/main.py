"""
APRetriever FastAPI Search Service - å®Œæ•´ç‰ˆæœ¬
"""

import os
import json
import re
import urllib.parse
import openai
from typing import List, Tuple

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from opensearchpy import OpenSearch

# å¯¼å…¥å·¥å…·æ¨¡å—
import sys
sys.path.append('.')
from utils.embedding import BGEM3Embedder
from utils.rerank import BGEReranker
from utils.query_expansion import expand_query
from utils.settings import settings
from utils.models import SearchRequest, SearchResult, SearchResponse
from utils.s3_cache import S3Cache

# ç¦ç”¨Pythonå­—èŠ‚ç ç¼“å­˜
os.environ['PYTHONDONTWRITEBYTECODE'] = '1'
os.environ['PYTHONUNBUFFERED'] = '1'

# è®¾ç½® OpenAI API é…ç½®
openai.api_key = os.getenv("OPENAI_API_KEY", "")
openai.api_base = os.getenv("OPENAI_API_BASE", "http://127.0.0.1:8000/v1")

class AsyncPaperSearch:
    """Asyncè®ºæ–‡æœç´¢å¼•æ“ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        self.opensearch_client = None
        self.embedder = None
        self.reranker = None
        self.cache = None
        
        # åˆå§‹åŒ–å„ä¸ªæœåŠ¡
        self._init_opensearch()
        self._init_embedder()
        self._init_reranker()
        self._init_cache()
    
    def _init_cache(self):
        """åˆå§‹åŒ–S3+DynamoDBç¼“å­˜"""
        try:
            if settings.USE_S3_CACHE:
                self.cache = S3Cache(
                    bucket_name=settings.S3_BUCKET_NAME,
                    table_name=settings.DYNAMODB_TABLE_NAME,
                    region_name=settings.DYNAMODB_REGION
                )
                print("âœ… S3+DynamoDBç¼“å­˜åˆå§‹åŒ–æˆåŠŸ")
            else:
                # å¦‚æœä¸ä½¿ç”¨S3ç¼“å­˜ï¼Œå¯ä»¥å›é€€åˆ°åŸæ¥çš„DynamoDBç¼“å­˜
                from utils.dynamodb_cache import DynamoDBCache
                self.cache = DynamoDBCache()
                print("âœ… DynamoDBç¼“å­˜åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ ç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {e}")
            self.cache = None
    
    def _init_opensearch(self):
        """åˆå§‹åŒ–OpenSearchå®¢æˆ·ç«¯"""
        if all([settings.OPENSEARCH_HOST, settings.OPENSEARCH_PORT, 
                settings.OPENSEARCH_USERNAME, settings.OPENSEARCH_PASSWORD]):
            try:
                self.opensearch_client = OpenSearch(
                    hosts=[{'host': settings.OPENSEARCH_HOST, 'port': settings.OPENSEARCH_PORT}],
                    http_auth=(settings.OPENSEARCH_USERNAME, settings.OPENSEARCH_PASSWORD),
                    use_ssl=True,
                    verify_certs=False,
                    ssl_show_warn=False
                )
                print("âœ… OpenSearchå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ OpenSearchå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            print("âš ï¸  OpenSearché…ç½®ä¸å®Œæ•´ï¼Œå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
    
    def _init_embedder(self):
        """åˆå§‹åŒ–BGE-M3åµŒå…¥æœåŠ¡"""
        if settings.SAGEMAKER_ENDPOINT_NAME:
            try:
                self.embedder = BGEM3Embedder(settings.SAGEMAKER_ENDPOINT_NAME)
                print("âœ… BGE-M3åµŒå…¥æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ BGE-M3åµŒå…¥æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            print("âš ï¸  æœªé…ç½®SAGEMAKER_ENDPOINT_NAMEï¼ŒåµŒå…¥æœåŠ¡ä¸å¯ç”¨")
    
    def _init_reranker(self):
        """åˆå§‹åŒ–BGEé‡æ’åºæœåŠ¡"""
        if settings.ENABLE_RERANK and settings.SAGEMAKER_ENDPOINT_RERANK_NAME:
            try:
                self.reranker = BGEReranker(settings.SAGEMAKER_ENDPOINT_RERANK_NAME)
                print("âœ… BGEé‡æ’åºæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ BGEé‡æ’åºæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            print("âš ï¸  é‡æ’åºæœåŠ¡æœªå¯ç”¨æˆ–æœªé…ç½®")
    
    def extract_matched_keywords(self, highlight: dict) -> List[str]:
        """æå–åŒ¹é…çš„å…³é”®è¯"""
        matched = []
        for field, highlights in highlight.items():
            for highlight_text in highlights:
                matches = re.findall(r'<em>(.*?)</em>', highlight_text)
                matched.extend(matches)
        return list(set(matched))

    def evaluate_relevance(self, query: str, text: str, enable_llm: bool = False) -> Tuple[bool, str]:
        """
        ä½¿ç”¨LLMè¯„ä¼°æ–‡æœ¬ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
        
        Args:
            query: æŸ¥è¯¢è¯
            text: å¾…è¯„ä¼°æ–‡æœ¬
            enable_llm: æ˜¯å¦å¯ç”¨LLMè¯„ä¼°ï¼Œé»˜è®¤ä¸ºFalse
            
        Returns:
            tuple: (æ˜¯å¦ç›¸å…³, ç›¸å…³åŸå› )
        """
        # å¦‚æœLLMç›¸å…³æ€§è¯„ä¼°è¢«ç¦ç”¨ï¼Œç›´æ¥è¿”å›Trueå’Œé»˜è®¤åŸå› 
        if not enable_llm:
            return True, "æœªå¯ç”¨LLMè¯„ä¼°"
            
        # å¦‚æœæ²¡æœ‰é…ç½®OpenAI APIï¼Œç›´æ¥è¿”å›True
        if not openai.api_key:
            return True, "æœªé…ç½®OpenAI API"
            
        try:
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŒ»å­¦æ–‡çŒ®ç­›é€‰åŠ©æ‰‹ã€‚æˆ‘ä¼šæä¾›ä¸€ä¸ªåŒ»å­¦topicå’Œä¸‹åˆ—åŒ»å­¦æ–‡çŒ®çš„æ‘˜è¦ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­è¿™ç¯‡æ–‡çŒ®æ˜¯å¦ä¸è¯¥ç–¾ç—…ç›¸å…³ï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹è§„åˆ™è¾“å‡ºç»“æœï¼š

å¦‚æœæ‘˜è¦ä¸»è¦è®¨è®ºè¯¥ç–¾ç—…ï¼Œè¿”å› trueï¼Œå¹¶ç»™å‡ºç†ç”±ï¼ˆä¸è¶…è¿‡15å­—ï¼‰ã€‚
å¦‚æœæ‘˜è¦ä¾§é¢æåˆ°è¯¥ç–¾ç—…ï¼ˆä¾‹å¦‚ä½œä¸ºèƒŒæ™¯ã€å¯¹æ¯”æˆ–æ¬¡è¦å†…å®¹ï¼‰ï¼Œè¿”å› trueï¼Œå¹¶ç»™å‡ºç†ç”±ï¼ˆä¸è¶…è¿‡15å­—ï¼‰ã€‚
å¦‚æœæ‘˜è¦ä¸è¯¥ç–¾ç—…å®Œå…¨æ— å…³ï¼Œè¿”å› falseï¼Œå¹¶ç»™å‡ºç†ç”±ï¼ˆä¸è¶…è¿‡15å­—ï¼‰ã€‚

åŒ»å­¦topicï¼š{query}
æ–‡çŒ®æ‘˜è¦ï¼š{text}

å§‹ç»ˆä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œå¹¶åŒ…è£¹åœ¨ <json></json> æ ‡ç­¾ä¸­ã€‚
JSON ç»“æ„å¦‚ä¸‹ï¼š{{
  "is_relevant": IF_RELEVANT,
  "reason": REASON
}}"""
            
            # ä½¿ç”¨OpenAI APIè¿›è¡Œè¯„ä¼°
            client = openai.OpenAI(api_key=openai.api_key, base_url=openai.api_base)
            response = client.chat.completions.create(
                model="Qwen2.5-72B-Instruct-AWQ",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦æ–‡çŒ®ç›¸å…³æ€§è¯„ä¼°ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·çš„è¦æ±‚æ ¼å¼è¾“å‡ºç»“æœã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.01,
                max_tokens=100
            )
            
            result = response.choices[0].message.content.strip()
            try:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…JSONå†…å®¹
                json_patterns = [
                    r'<json>(.*?)</json>',  # åŒ¹é…<json>æ ‡ç­¾ä¸­çš„å†…å®¹
                    r'```json\s*(.*?)\s*```'  # åŒ¹é…```json ```ä¸­çš„å†…å®¹
                ]
                
                json_str = None
                for pattern in json_patterns:
                    matches = re.findall(pattern, result, re.DOTALL)
                    if matches:
                        json_str = matches[0].strip()
                        break
                
                if not json_str:
                    print(f"æœªæ‰¾åˆ°JSONå†…å®¹: {result}")
                    return True, "JSONå†…å®¹è§£æå¤±è´¥"
                
                json_result = json.loads(json_str)
                return json_result.get('is_relevant', True), json_result.get('reason', 'è§£æè¯„ä¼°åŸå› å¤±è´¥')
            except json.JSONDecodeError:
                print(f"JSONè§£æå¤±è´¥: {result}")
                return True, "JSONè§£æå¤±è´¥"
        except Exception as e:
            print(f"ç›¸å…³æ€§è¯„ä¼°å‡ºé”™: {str(e)}")
            return True, f"è¯„ä¼°å‡ºé”™: {str(e)}"  # å‡ºé”™æ—¶é»˜è®¤ä¿ç•™è¯¥ç»“æœ

    def build_hybrid_search_query(self, query_terms: List[str], query_embedding: List[float], page: int, page_size: int) -> dict:
        """æ„å»ºæ··åˆæœç´¢æŸ¥è¯¢"""
        from_value = (page - 1) * page_size
        
        # æ„å»ºå¤šä¸ªæŸ¥è¯¢è¯çš„boolæŸ¥è¯¢ï¼Œç±»ä¼¼å…³é”®è¯æœç´¢
        should_queries = []
        for keyword in query_terms:
            should_queries.extend([
                {
                    "match_phrase": {
                        "title": {
                            "query": keyword,
                            "boost": 5  # æ ‡é¢˜æƒé‡
                        }
                    }
                },
                {
                    "match_phrase": {
                        "keywords": {
                            "query": keyword,
                            "boost": 10  # å…³é”®è¯æƒé‡
                        }
                    }
                },
                {
                    "match_phrase": {
                        "abstract": {
                            "query": keyword,
                            "boost": 2  # æ‘˜è¦æƒé‡
                        }
                    }
                }
            ])
        
        return {
            "from": from_value,
            "size": page_size,
            "query": {
                "hybrid": {
                    "queries": [
                        {
                            "bool": {
                                "should": should_queries,
                                "minimum_should_match": 1  # è‡³å°‘åŒ¹é…ä¸€ä¸ªå…³é”®è¯
                            }
                        },
                        {
                            "knn": {
                                "embedding": {
                                    "vector": query_embedding,
                                    "k": page_size
                                }
                            }
                        }
                    ]
                }
            },
            "_source": ["id", "title", "abstract", "keywords"],
            "highlight": {
                "fields": {
                    "title": {},
                    "abstract": {},
                    "keywords": {}
                }
            }
        }

    def build_vector_search_query(self, query_embedding: List[float], query_terms: List[str], page: int, page_size: int) -> dict:
        """æ„å»ºå‘é‡æœç´¢æŸ¥è¯¢"""
        from_value = (page - 1) * page_size
        
        # æ„å»ºmust_notæŸ¥è¯¢
        must_not_queries = []
        for keyword in query_terms:
            must_not_queries.extend([
                {"match_phrase": {"title": keyword}},
                {"match_phrase": {"keywords": keyword}},
                {"match_phrase": {"abstract": keyword}}
            ])

        return {
            "from": from_value,
            "size": page_size,
            "query": {
                "bool": {
                    "must": {
                        "knn": {
                            "embedding": {
                                "vector": query_embedding,
                                "k": page_size
                            }
                        }
                    },
                    "must_not": must_not_queries
                }
            },
            "_source": ["id", "title", "abstract", "keywords"]
        }

    def build_keyword_search_query(self, query_terms: List[str], page: int, page_size: int) -> dict:
        """æ„å»ºå…³é”®è¯æœç´¢æŸ¥è¯¢"""
        from_value = (page - 1) * page_size
        
        # æ„å»ºboolæŸ¥è¯¢
        should_queries = []
        for keyword in query_terms:
            should_queries.extend([
                {
                    "match_phrase": {
                        "title": {
                            "query": keyword,
                            "boost": 5  # æ ‡é¢˜æƒé‡
                        }
                    }
                },
                {
                    "match_phrase": {
                        "keywords": {
                            "query": keyword,
                            "boost": 10  # å…³é”®è¯æƒé‡
                        }
                    }
                },
                {
                    "match_phrase": {
                        "abstract": {
                            "query": keyword,
                            "boost": 2  # æ‘˜è¦æƒé‡
                        }
                    }
                }
            ])
        
        return {
            "from": from_value,
            "size": page_size,
            "query": {
                "bool": {
                    "should": should_queries,
                    "minimum_should_match": 1  # è‡³å°‘åŒ¹é…ä¸€ä¸ªå…³é”®è¯
                }
            },
            "_source": ["id", "title", "abstract", "keywords"],
            "highlight": {
                "fields": {
                    "title": {},
                    "abstract": {},
                    "keywords": {}
                }
            }
        }

    def find_matched_keywords(self, keywords: List[str], hit_source: dict) -> List[str]:
        """æ‰¾å‡ºåŒ¹é…çš„å…³é”®è¯"""
        matched_keywords = []
        for keyword in keywords:
            # æ£€æŸ¥æ ‡é¢˜ã€å…³é”®è¯å’Œæ‘˜è¦ä¸­æ˜¯å¦åŒ…å«è¯¥å…³é”®è¯
            if (keyword.lower() in hit_source['title'].lower() or
                keyword.lower() in ' '.join(hit_source.get('keywords', [])).lower() or
                keyword.lower() in hit_source['abstract'].lower()):
                matched_keywords.append(keyword)
        return matched_keywords

    async def search(self, request: SearchRequest) -> SearchResponse:
        """æ‰§è¡Œæœç´¢"""
        try:
            # æ£€æŸ¥OpenSearchæ˜¯å¦å¯ç”¨
            if not self.opensearch_client:
                raise Exception("OpenSearchæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®")
            
            # è§£å†³URLç¼–ç é—®é¢˜
            try:
                query = urllib.parse.unquote(request.query)
                print(f"è§£ç åçš„æŸ¥è¯¢: {query}")
                print(f"LLMç›¸å…³æ€§è¯„ä¼°: {'å¯ç”¨' if request.enableLlm else 'ç¦ç”¨'}")
            except Exception as e:
                print(f"URLè§£ç å‡ºé”™: {str(e)}")
                query = request.query
            
            if not query:
                return SearchResponse(
                    total=0,
                    results=[],
                    searchType=request.searchType,
                    rewrittenTerms=None
                )
            
            print(f"æœç´¢ç±»å‹: {request.searchType}, æŸ¥è¯¢: {query}")
            
            # æŸ¥è¯¢æ‰©å±•
            query_terms = expand_query(query)
            print(f"æ‰©å±•åçš„æœç´¢è¯: {query_terms}")
            
            if request.searchType == 'hybrid':
                return await self._hybrid_search(query, query_terms, request)
            elif request.searchType == 'vector':
                return await self._vector_search(query, query_terms, request)
            else:
                return await self._keyword_search(query, query_terms, request)
            
        except Exception as e:
            print(f"æœç´¢é”™è¯¯: {str(e)}")
            return SearchResponse(
                total=0,
                results=[],
                searchType=request.searchType,
                rewrittenTerms=None
            )

    async def _hybrid_search(self, query: str, query_terms: List[str], request: SearchRequest) -> SearchResponse:
        """æ··åˆæœç´¢"""
        if not self.embedder:
            raise Exception("åµŒå…¥æœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œæ··åˆæœç´¢")
            
        query_embedding = self.embedder.get_embeddings(query)[0].tolist()
        search_query = self.build_hybrid_search_query(query_terms, query_embedding, request.page, request.pageSize)
        
        # ä½¿ç”¨search pipelineè¿›è¡Œå½’ä¸€åŒ–å’Œæƒé‡ç»„åˆ
        search_params = {"search_pipeline": "nlp-search-pipeline"}
        
        # æ‰§è¡Œæœç´¢
        response = self.opensearch_client.search(
            index=settings.OPENSEARCH_INDEX_NAME,
            body=search_query,
            params=search_params
        )
        
        # å¤„ç†æœç´¢ç»“æœ
        results = []
        for hit in response['hits']['hits']:
            # æ‰¾å‡ºåŒ¹é…çš„å…³é”®è¯
            matched_keywords = self.find_matched_keywords(query_terms, hit['_source'])
            
            # æå–é«˜äº®åŒ¹é…çš„å…³é”®è¯
            highlight_matched = self.extract_matched_keywords(hit.get('highlight', {}))
            
            result = SearchResult(
                id=hit['_source']['id'],
                title=hit['_source']['title'],
                keywords=hit['_source'].get('keywords', []),
                abstract=hit['_source']['abstract'],
                score=hit['_score'],
                source='hybrid',
                matched_keywords=matched_keywords + highlight_matched
            )
            results.append(result)
        
        # å¦‚æœå¯ç”¨äº†rerankåŠŸèƒ½ï¼Œå¯¹æ‰€æœ‰ç»“æœè¿›è¡Œé‡æ’åº
        if settings.ENABLE_RERANK and self.reranker and results:
            results = await self._rerank_results(query, results)
        
        # åˆ›å»ºæœç´¢å“åº”
        search_response = SearchResponse(
            total=response['hits']['total']['value'],
            results=results,
            searchType=request.searchType,
            rewrittenTerms=query_terms
        )
        
        # ä¿å­˜æœç´¢ç»“æœåˆ°DynamoDBç¼“å­˜
        if self.cache:
            search_id = self.cache.save_search_result(
                query=query,
                search_type=request.searchType,
                response=search_response,
                enable_llm=request.enableLlm
            )
            if search_id:
                search_response.search_id = search_id
        
        return search_response

    async def _vector_search(self, query: str, query_terms: List[str], request: SearchRequest) -> SearchResponse:
        """å‘é‡æœç´¢"""
        if not self.embedder:
            raise Exception("åµŒå…¥æœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œå‘é‡æœç´¢")
            
        query_embedding = self.embedder.get_embeddings(query)[0].tolist()
        search_query = self.build_vector_search_query(query_embedding, query_terms, request.page, request.pageSize)
        
        # æ‰§è¡Œæœç´¢
        response = self.opensearch_client.search(
            index=settings.OPENSEARCH_INDEX_NAME,
            body=search_query
        )
        
        # å¤„ç†æœç´¢ç»“æœ
        results = []
        for hit in response['hits']['hits']:
            # æ„å»ºè¯„ä¼°æ–‡æœ¬
            eval_text = f"æ ‡é¢˜ï¼š{hit['_source']['title']}\næ‘˜è¦ï¼š{hit['_source']['abstract']}"
            
            # è¿›è¡Œç›¸å…³æ€§è¯„ä¼°
            is_relevant, reason = self.evaluate_relevance(query, eval_text, request.enableLlm)
            if is_relevant:
                result = SearchResult(
                    id=hit['_source']['id'],
                    title=hit['_source']['title'],
                    keywords=hit['_source'].get('keywords', []),
                    abstract=hit['_source']['abstract'],
                    score=hit['_score'],
                    source='vector',
                    relevance_reason=reason
                )
                results.append(result)
            else:
                print(f"æ–‡æ¡£ {hit['_source']['id']} è¢«LLMè¯„ä¼°ä¸ºä¸ç›¸å…³ï¼ŒåŸå› ï¼š{reason}")
        
        # åˆ›å»ºæœç´¢å“åº”
        search_response = SearchResponse(
            total=len(results),
            results=results,
            searchType=request.searchType,
            rewrittenTerms=query_terms
        )
        
        # ä¿å­˜æœç´¢ç»“æœåˆ°DynamoDBç¼“å­˜
        if self.cache:
            search_id = self.cache.save_search_result(
                query=query,
                search_type=request.searchType,
                response=search_response,
                enable_llm=request.enableLlm
            )
            if search_id:
                search_response.search_id = search_id
        
        return search_response

    async def _keyword_search(self, query: str, query_terms: List[str], request: SearchRequest) -> SearchResponse:
        """å…³é”®è¯æœç´¢"""
        search_query = self.build_keyword_search_query(query_terms, request.page, request.pageSize)
        
        # æ‰§è¡Œæœç´¢
        response = self.opensearch_client.search(
            index=settings.OPENSEARCH_INDEX_NAME,
            body=search_query
        )
        
        # å¤„ç†æœç´¢ç»“æœ
        results = []
        seen_ids = set()  # ç”¨äºè®°å½•å·²ç»å¬å›çš„æ–‡æ¡£ID
        
        for hit in response['hits']['hits']:
            # æ‰¾å‡ºåŒ¹é…çš„å…³é”®è¯
            matched_keywords = self.find_matched_keywords(query_terms, hit['_source'])
            
            # æå–é«˜äº®åŒ¹é…çš„å…³é”®è¯
            highlight_matched = self.extract_matched_keywords(hit.get('highlight', {}))
            
            result = SearchResult(
                id=hit['_source']['id'],
                title=hit['_source']['title'],
                keywords=hit['_source'].get('keywords', []),
                abstract=hit['_source']['abstract'],
                score=hit['_score'],
                source='keyword',
                matched_keywords=matched_keywords + highlight_matched
            )
            results.append(result)
            seen_ids.add(hit['_source']['id'])
        
        # å¦‚æœå…³é”®è¯æœç´¢ç»“æœä¸è¶³10000ä¸”æœ‰åµŒå…¥æœåŠ¡ï¼Œä½¿ç”¨å‘é‡æ£€ç´¢è¡¥å……
        if len(results) < 10000 and self.embedder:
            results = await self._supplement_with_vector_search(query, query_terms, results, seen_ids, request)
        
        # å¦‚æœå¯ç”¨äº†rerankåŠŸèƒ½ï¼Œå¯¹æ‰€æœ‰ç»“æœè¿›è¡Œé‡æ’åº
        if settings.ENABLE_RERANK and self.reranker and results:
            results = await self._rerank_results(query, results)
        
        # åˆ›å»ºæœç´¢å“åº”
        search_response = SearchResponse(
            total=len(results),
            results=results,
            searchType=request.searchType,
            rewrittenTerms=query_terms
        )
        
        # ä¿å­˜æœç´¢ç»“æœåˆ°DynamoDBç¼“å­˜
        if self.cache:
            search_id = self.cache.save_search_result(
                query=query,
                search_type=request.searchType,
                response=search_response,
                enable_llm=request.enableLlm
            )
            if search_id:
                search_response.search_id = search_id
        
        return search_response

    async def _supplement_with_vector_search(self, query: str, query_terms: List[str], 
                                           results: List[SearchResult], seen_ids: set, 
                                           request: SearchRequest) -> List[SearchResult]:
        """ä½¿ç”¨å‘é‡æœç´¢è¡¥å……å…³é”®è¯æœç´¢ç»“æœ"""
        print(f"å…³é”®è¯æœç´¢ç»“æœæ•°é‡ä¸è¶³10000ï¼ˆå½“å‰ï¼š{len(results)}ï¼‰ï¼Œä½¿ç”¨å‘é‡æ£€ç´¢è¡¥å……")
        remaining_size = 10000 - len(results)
        
        # å°†åŸå§‹æŸ¥è¯¢å’Œæ‰©å†™å…³é”®è¯åˆå¹¶æˆä¸€ä¸ªæŸ¥è¯¢
        combined_query = " ".join([query] + query_terms)
        print(f"åˆå¹¶åçš„æŸ¥è¯¢è¯: {combined_query}")
        
        # è·å–åˆå¹¶æŸ¥è¯¢çš„å‘é‡è¡¨ç¤º
        query_embedding = self.embedder.get_embeddings(combined_query)[0].tolist()
        
        # æ„å»ºmust_notæŸ¥è¯¢ï¼Œæ’é™¤å·²æœ‰ç»“æœå’ŒåŒ…å«å…³é”®è¯çš„æ–‡æ¡£
        must_not_queries = [{"terms": {"id": list(seen_ids)}}]  # æ’é™¤å·²æœ‰ç»“æœ
        
        # æ·»åŠ å…³é”®è¯åŒ¹é…æ’é™¤
        for keyword in query_terms:
            must_not_queries.extend([
                {"match_phrase": {"title": keyword}},
                {"match_phrase": {"keywords": keyword}},
                {"match_phrase": {"abstract": keyword}}
            ])
        
        # æ„å»ºå‘é‡æ£€ç´¢æŸ¥è¯¢
        vector_search_query = {
            "size": remaining_size,
            "query": {
                "bool": {
                    "must": {
                        "knn": {
                            "embedding": {
                                "vector": query_embedding,
                                "k": remaining_size
                            }
                        }
                    },
                    "must_not": must_not_queries
                }
            },
            "_source": ["id", "title", "abstract", "keywords"]
        }
        
        # æ‰§è¡Œå‘é‡æ£€ç´¢
        vector_response = self.opensearch_client.search(
            index=settings.OPENSEARCH_INDEX_NAME,
            body=vector_search_query
        )
        
        # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
        vector_results = []
        for hit in vector_response['hits']['hits']:
            if hit['_source']['id'] not in seen_ids and hit['_score'] >= 0.1:  # åªæ·»åŠ åˆ†æ•°å¤§äº0.1çš„ç»“æœ
                # æ„å»ºè¯„ä¼°æ–‡æœ¬
                eval_text = f"æ ‡é¢˜ï¼š{hit['_source']['title']}\næ‘˜è¦ï¼š{hit['_source']['abstract']}"
                
                # è¿›è¡Œç›¸å…³æ€§è¯„ä¼°
                is_relevant, reason = self.evaluate_relevance(query, eval_text, request.enableLlm)
                if is_relevant:
                    result = SearchResult(
                        id=hit['_source']['id'],
                        title=hit['_source']['title'],
                        keywords=hit['_source'].get('keywords', []),
                        abstract=hit['_source']['abstract'],
                        score=hit['_score'],
                        source='vector',
                        relevance_reason=reason
                    )
                    vector_results.append(result)
                else:
                    print(f"æ–‡æ¡£ {hit['_source']['id']} è¢«LLMè¯„ä¼°ä¸ºä¸ç›¸å…³ï¼ŒåŸå› ï¼š{reason}")
        
        # æ·»åŠ å‘é‡æ£€ç´¢ç»“æœåˆ°æœ€ç»ˆç»“æœä¸­
        results.extend(vector_results)
        
        print(f"å‘é‡æ£€ç´¢ç»“æœæ•°: {len(vector_results)}")
        print(f"è¡¥å……åçš„ç»“æœæ€»æ•°ï¼š{len(results)}")
        
        # å¯¹æ‰€æœ‰ç»“æœæŒ‰åˆ†æ•°é‡æ–°æ’åº
        results.sort(key=lambda x: x.score, reverse=True)
        print(f"æ’åºåçš„å‰3ä¸ªç»“æœå¾—åˆ†: {[result.score for result in results[:3]]}")
        
        return results

    async def _rerank_results(self, query: str, results: List[SearchResult]) -> List[SearchResult]:
        """å¯¹æœç´¢ç»“æœè¿›è¡Œé‡æ’åº"""
        try:
            # æå–æ‰€æœ‰æ–‡æ¡£çš„æ ‡é¢˜
            titles = [result.title for result in results]
            
            # ä½¿ç”¨åŸå§‹æŸ¥è¯¢å’Œæ ‡é¢˜è¿›è¡Œrerank
            rerank_results = self.reranker.rerank(query, titles)
            
            # æ›´æ–°ç»“æœçš„å¾—åˆ†
            if rerank_results:
                for i, rerank_result in enumerate(rerank_results):
                    if i < len(results):
                        results[i].score = rerank_result['score']
                
                # æ ¹æ®æ–°çš„å¾—åˆ†é‡æ–°æ’åº
                results.sort(key=lambda x: x.score, reverse=True)
                print(f"Rerankå®Œæˆï¼Œé‡æ–°æ’åºåçš„å‰3ä¸ªç»“æœå¾—åˆ†: {[result.score for result in results[:3]]}")
        except Exception as e:
            print(f"Rerankè¿‡ç¨‹å‡ºé”™: {str(e)}")
        
        return results

    def get_service_status(self) -> dict:
        """è·å–æœåŠ¡çŠ¶æ€"""
        cache_type = "none"
        if self.cache:
            if hasattr(self.cache, 's3_client'):
                cache_type = "s3+dynamodb"
            else:
                cache_type = "dynamodb"
        
        return {
            "opensearch": self.opensearch_client is not None,
            "embedder": self.embedder is not None,
            "reranker": self.reranker is not None,
            "cache": self.cache is not None,
            "cache_type": cache_type
        }

# ================================
# å…¨å±€æœç´¢å¼•æ“å®ä¾‹
# ================================

search_engine = AsyncPaperSearch()

# ================================
# FastAPIåº”ç”¨
# ================================

app = FastAPI(
    title=settings.PROJECT_NAME,
    description="åŒ»å­¦æ–‡çŒ®æœç´¢APIæœåŠ¡ - å®Œæ•´ç‰ˆæœ¬",
    version=settings.VERSION,
    openapi_url="/openapi.json" if settings.ENVIRONMENT != "production" else None
)

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_HOSTS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ================================
# APIç«¯ç‚¹
# ================================

@app.get("/")
async def root():
    """æ ¹ç«¯ç‚¹"""
    return {
        "message": "Async FastAPI Search Service",
        "version": settings.VERSION,
        "docs_url": "/docs",
        "health_url": "/health"
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    status = {
        "status": "healthy",
        "service": "async-fastapi-search",
        "version": settings.VERSION,
        "services": search_engine.get_service_status()
    }
    return status

@app.post("/search", response_model=SearchResponse)
async def search_post(request: SearchRequest):
    """POSTæ–¹å¼æœç´¢"""
    return await search_engine.search(request)

@app.get("/cache/{search_id}")
async def get_cached_search(search_id: str):
    """æ ¹æ®æœç´¢IDè·å–ç¼“å­˜çš„æœç´¢ç»“æœ"""
    if not search_engine.cache:
        return {"error": "ç¼“å­˜æœªåˆå§‹åŒ–"}
    
    result = search_engine.cache.get_search_result(search_id)
    if result:
        return result
    else:
        return {"error": f"æœªæ‰¾åˆ°æœç´¢ID: {search_id}"}

@app.get("/cache/{search_id}/metadata")
async def get_cached_search_metadata(search_id: str):
    """æ ¹æ®æœç´¢IDè·å–ç¼“å­˜çš„æœç´¢å…ƒæ•°æ®ï¼ˆä¸åŒ…å«å®Œæ•´ç»“æœï¼‰"""
    if not search_engine.cache:
        return {"error": "ç¼“å­˜æœªåˆå§‹åŒ–"}
    
    # æ£€æŸ¥æ˜¯å¦æœ‰get_search_metadataæ–¹æ³•ï¼ˆS3Cacheæœ‰ï¼ŒDynamoDBCacheæ²¡æœ‰ï¼‰
    if hasattr(search_engine.cache, 'get_search_metadata'):
        result = search_engine.cache.get_search_metadata(search_id)
    else:
        result = search_engine.cache.get_search_result(search_id)
    
    if result:
        return result
    else:
        return {"error": f"æœªæ‰¾åˆ°æœç´¢ID: {search_id}"}

@app.get("/cache/stats")
async def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    if not search_engine.cache:
        return {"error": "ç¼“å­˜æœªåˆå§‹åŒ–"}
    
    return search_engine.cache.get_cache_stats()

@app.delete("/cache/{search_id}")
async def delete_cached_search(search_id: str):
    """åˆ é™¤ç¼“å­˜çš„æœç´¢ç»“æœ"""
    if not search_engine.cache:
        return {"error": "ç¼“å­˜æœªåˆå§‹åŒ–"}
    
    success = search_engine.cache.delete_search_result(search_id)
    if success:
        return {"message": f"æœç´¢ç»“æœå·²åˆ é™¤: {search_id}"}
    else:
        return {"error": f"åˆ é™¤å¤±è´¥: {search_id}"}

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ å¯åŠ¨ Async FastAPI Search Service...")
    print("ğŸ“– APIæ–‡æ¡£: http://localhost:8000/docs")
    print("ğŸ” æœç´¢API: http://localhost:8000/search")
    print("â¤ï¸  å¥åº·æ£€æŸ¥: http://localhost:8000/health")
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True) 